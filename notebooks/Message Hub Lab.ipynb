{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Lab Sample - Working with IBM Cloud Message Hub with REST API.\n",
    "\n",
    "IBM Message Hub is a scalable, distributed, high throughput message bus to integrate your on-premise and off-premise cloud technologies. \n",
    "You can wire micro-services together using open protocols, connect stream data to analytics to realise powerful insight and feed event data to multiple applications to react in real time.\n",
    "\n",
    "This lab will introduce you to working with MessageHub   It will be written in Python and run in IBM's Data Science Experience environment through a Jupyter notebook.  While you work, it will be valuable to reference the [Apache Spark Documentation](http://spark.apache.org/docs/latest/programming-guide.html).  Since it is Python, be careful of whitespace!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The basics\n",
    "Message Hub acts like a real-time messaging system or data pipeline used for all incoming and outgoing communication. The most important terms are:\n",
    "- topic\n",
    "- producer\n",
    "- consumer\n",
    "- cluster\n",
    "\n",
    "A producer sends a message to a specific topic within a Kafka cluster and a consumer receives this message by subscribing to this topic.\n",
    "\n",
    "Consumers can join what is called a Consumer Group. Within the group, only one consumer will get to process the message. Grouping consumers like that is useful for parallel processing of messages in high-throughput applications.\n",
    "\n",
    "<img src='https://cdn-images-1.medium.com/max/1600/0*VD_oFFWFrE3QfDGu.png'></img>\n",
    "(Fig 1: Basic architecture, Kafka Documentation)\n",
    "\n",
    "The REST API allows you to define consumer instances and consumer groups.\n",
    "\n",
    "Content for this notebook is prepared from following IBM DSX blog \n",
    "https://medium.com/ibm-data-science-experience/receive-messages-from-ibm-message-hub-c52ebdedcfb9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step:1 Get Service credentials for provisioned Message Hub Service into notebook.\n",
    "You can do that rom within a notebook, pick that connection under Data panel and connections section\n",
    "Click Insert to Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/smatlapudi/DSX-Training-Labs/raw/master/images/add_connection_to_notebook.png' width=\"60%\" height=\"60%\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "credentials_1 = {\n",
    "  'instance_id':'d501d02b-234f-458a-8592-7080ad2a0e3b',\n",
    "  'mqlight_lookup_url':'https://mqlight-lookup-prod02.messagehub.services.us-south.bluemix.net/Lookup?serviceId=d501d02b-234f-458a-8592-7080ad2a0e3b',\n",
    "  'api_key':'9IyKpJoLKeB3zfPGQs3UUQHDzy6Oicx03EuDl9epGR8elWIi',\n",
    "  'kafka_admin_url':'https://kafka-admin-prod02.messagehub.services.us-south.bluemix.net:443',\n",
    "  'kafka_rest_url':'https://kafka-rest-prod02.messagehub.services.us-south.bluemix.net:443',\n",
    "  'kafka_brokers_sasl':'kafka05-prod02.messagehub.services.us-south.bluemix.net:9093,kafka04-prod02.messagehub.services.us-south.bluemix.net:9093,kafka01-prod02.messagehub.services.us-south.bluemix.net:9093,kafka02-prod02.messagehub.services.us-south.bluemix.net:9093,kafka03-prod02.messagehub.services.us-south.bluemix.net:9093',\n",
    "  'user':'9IyKpJoLKeB3zfPG',\n",
    "  'password':\"\"\"Qs3UUQHDzy6Oicx03EuDl9epGR8elWIi\"\"\",\n",
    "  'topic':'sfo.lab'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Write Messages to Message Hub Topic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1) Use python requests library for making http calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade requests\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2) Import json library and setup http header values for REST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# credentials_1 in below lines is comming from code insert for message hub connection.\n",
    "# you may need to change this depending on the credentials name inserted into the code in step#1\n",
    "authToken = credentials_1['api_key'] \n",
    "kafkaRestUrl = credentials_1['kafka_rest_url']\n",
    "headers = {  \n",
    "  'X-Auth-Token': authToken,\n",
    "  'Content-Type': 'application/json',\n",
    "  'Accept': 'application/vnd.kafka.v1+json, application/vnd.kafka+json, application/json'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3) Prepare http body with  messages to publish\n",
    "\n",
    "The values submitted in the records array have to be binary encoded for our consumer instance. JSON or Avro encoding is not currently supported with Message Hub REST API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import binascii\n",
    "\n",
    "body = json.dumps({  \n",
    "  'records': [\n",
    "    {'key': 'key-1','value': binascii.hexlify(b\"value one\").decode('utf-8')},\n",
    "    {'key': 'key-2','value': binascii.hexlify(b\"value two\").decode('utf-8')},\n",
    "    {'key': 'key-3','value': binascii.hexlify(b\"value three\").decode('utf-8')},\n",
    "    {'key': 'key-4','value': binascii.hexlify(b\"value four\").decode('utf-8')},\n",
    "    {'key': 'key-5','value': binascii.hexlify(b\"value five\").decode('utf-8')},\n",
    "    {'key': 'key-6','value': binascii.hexlify(b\"value six\").decode('utf-8')},\n",
    "    {'key': 'key-7','value': binascii.hexlify(b\"value seven\").decode('utf-8')},\n",
    "    {'key': 'key-8','value': binascii.hexlify(b\"value eight\").decode('utf-8')},\n",
    "    {'key': 'key-9','value': binascii.hexlify(b\"value nine\").decode('utf-8')},\n",
    "    {'key': 'key-10','value': binascii.hexlify(b\"value ten\").decode('utf-8')},\n",
    "  ]\n",
    "}, ensure_ascii=False).encode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4) Make the REST call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kafkaTopic = 'sfo_mhub_lab'\n",
    "url = kafkaRestUrl + \"/topics/\" + kafkaTopic\n",
    "response = requests.post(url, data=body, headers=headers)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step:3 Read Messages from Message Hub Topic using Message Hub REST API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1) Create a Kafka consumer group & instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give a unique value for instance and group to avoid conflicts\n",
    "consumerInstance = 'instance11'  \n",
    "consumerGroup = 'group11'\n",
    "\n",
    "authToken = credentials_1['api_key']  \n",
    "kafkaRestUrl = credentials_1['kafka_rest_url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "body2 = json.dumps({  \n",
    "  'name': consumerInstance,\n",
    "  'format': 'binary',\n",
    "  'auto.offset.reset': 'smallest'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following method is used to submit a post request to the Kafka Rest API to create the consumer instance. It expects a valid authentication header and the consumer group and instance names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setConsumerInstanceAndGroup(consumerGroup, body1, headers):\n",
    "  response = requests.post(kafkaRestUrl + \"/consumers/\" + consumerGroup, data=body1, headers=headers)\n",
    "  print(response.status_code, response.reason, response.text)\n",
    "  result = response.json()\n",
    "  print(result)\n",
    "  consumerUrl = result['base_uri']\n",
    "  print(consumerUrl)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is usefullto delete the existing consumer instance and consumer group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteConsumerInstance(consumerGroup, consumerInstance, headers):\n",
    "  response = requests.delete(kafkaRestUrl + \"/consumers/\" + consumerGroup + \"/instances/\" + consumerInstance, headers=headers)\n",
    "  print(response.status_code, response.reason, response.text)\n",
    "  return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute setConsumerInstanceAndGroup method once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setConsumerInstanceAndGroup(consumerGroup, body2, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method below defines a loop we want to run for some amount of time. We eventually have to stop it in order to process the results. What we have here is not an actual streaming receiver as needed for Spark Streaming. We only demonstrate Spark core functionality by reading messages from Message Hub into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMessageFromKafka(maxArrayLength, maxIterations, consumerUrl, headers):  \n",
    "  results = []\n",
    "  length = 0\n",
    "  iteration = 0\n",
    "  while (length < maxArrayLength):\n",
    "    if (iteration > maxIterations): break\n",
    "\n",
    "    response = requests.get(kafkaRestUrl + \"/consumers/\"+consumerGroup+\"/instances/\"+consumerInstance+\"/topics/\"+kafkaTopic, headers=headers)        \n",
    "\n",
    "    print (response, response.reason, response.text)\n",
    "    data = response.text \n",
    "\n",
    "    x = json.loads(data)\n",
    "    length = length + len(x)\n",
    "    iteration = iteration + 1\n",
    "\n",
    "    print ('===============================')\n",
    "    print ('Number of incoming messages: ', len(x))\n",
    "    print ('===============================')\n",
    "\n",
    "    for obj in x:\n",
    "      value = binascii.unhexlify(obj['value']).decode('utf-8')\n",
    "\n",
    "      print(value)\n",
    "      results.append(value)\n",
    "\n",
    "  return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the method to receive all messages currently on the topic. Other parameters are set to limit the length of the message array and the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxArrayLength = 100  \n",
    "maxIterations = 5 \n",
    "\n",
    "results = getMessageFromKafka (maxArrayLength, maxIterations, url, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally delete consumer instance and group to avoid conflicts with other executions of this test code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteConsumerInstance(consumerGroup, consumerInstance, headers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next: Read Messages from Message Hub Topic using Spark Streaming Kafka client.\n",
    "Advanced exercise. Code not included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
